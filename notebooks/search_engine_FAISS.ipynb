{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35151e9f",
   "metadata": {},
   "source": [
    "# **Implementing a Semantic Search Engine with FAISS for Legal Documents**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bee742d",
   "metadata": {},
   "source": [
    "### **Setup and Import Dependencies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0099ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import faiss\n",
    "import torch\n",
    "import docx\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.preprocessing import normalize\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8cd68f5",
   "metadata": {},
   "source": [
    "### **Load and Prepare Documents**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d5c411",
   "metadata": {},
   "source": [
    "\n",
    "The process begins with document ingestion and preparation:\n",
    "\n",
    "- **Loading Documents**: The system loads DOCX files containing various legal documents\n",
    "- **Text Extraction**: Text is extracted from each document using the docx library\n",
    "- **Text Preprocessing**: Raw text is cleaned by removing extra whitespace\n",
    "- **Document Organization**: Documents are categorized (Employment Contracts, Privacy Policies, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a02ead1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not find documents file at ./model_output/documents.pkl\n",
      "Created new documents DataFrame with 25 documents\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>General Employment Agreement</td>\n",
       "      <td>Employment Contracts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Non-Compete Clause</td>\n",
       "      <td>Employment Contracts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Confidentiality Agreement</td>\n",
       "      <td>Employment Contracts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Termination Conditions</td>\n",
       "      <td>Employment Contracts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Intellectual Property Rights</td>\n",
       "      <td>Employment Contracts</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                          name              category\n",
       "0   0  General Employment Agreement  Employment Contracts\n",
       "1   1            Non-Compete Clause  Employment Contracts\n",
       "2   2     Confidentiality Agreement  Employment Contracts\n",
       "3   3        Termination Conditions  Employment Contracts\n",
       "4   4  Intellectual Property Rights  Employment Contracts"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function to load document data (assuming you have the documents DataFrame from previous work)\n",
    "def load_document_data(data_path='./model_output/documents.pkl'):\n",
    "    \"\"\"Load the documents DataFrame saved from previous steps.\"\"\"\n",
    "    try:\n",
    "        return pd.read_pickle(data_path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Could not find documents file at {data_path}\")\n",
    "        return None\n",
    "\n",
    "# Function to extract text from DOCX files\n",
    "def extract_text_from_docx(file_path):\n",
    "    \"\"\"Extract text content from a .docx file.\"\"\"\n",
    "    try:\n",
    "        doc = docx.Document(file_path)\n",
    "        full_text = []\n",
    "        for para in doc.paragraphs:\n",
    "            if para.text.strip():  # Skip empty paragraphs\n",
    "                full_text.append(para.text)\n",
    "        return '\\n'.join(full_text)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "# Function to preprocess text\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Clean and preprocess text.\"\"\"\n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    # Lowercase (optional for legal documents, as case might be significant)\n",
    "    # text = text.lower()\n",
    "    return text\n",
    "\n",
    "# Load documents DataFrame\n",
    "docs_df = load_document_data()\n",
    "\n",
    "# If the DataFrame doesn't exist or we need to create a new one from raw files\n",
    "if docs_df is None:\n",
    "    # Define document categories (same as in your classifier)\n",
    "    document_categories = {\n",
    "        \"Employment Contracts\": [\"General Employment Agreement\", \"Non-Compete Clause\", \n",
    "                               \"Confidentiality Agreement\", \"Termination Conditions\", \n",
    "                               \"Intellectual Property Rights\"],\n",
    "        \"Privacy Policies\": [\"General Privacy Policy\", \"Data Handling and Retention Policy\", \n",
    "                           \"Cookie Policy\", \"Employee Data Protection Agreement\", \n",
    "                           \"Customer Data Consent Form\"],\n",
    "        \"Corporate Governance\": [\"Bylaws Articles of Association\", \"Board of Directors Responsibilities\", \n",
    "                               \"Shareholders Agreement\", \"Conflict of Interest Policy\", \n",
    "                               \"Code of Ethics\"],\n",
    "        \"Commercial Agreements\": [\"Vendor Agreement\", \"Sales Contract\", \n",
    "                                \"Non-Disclosure Agreement\", \"Service Level Agreement\", \n",
    "                                \"Partnership Agreement\"],\n",
    "        \"Health and Safety\": [\"Workplace Safety Policy\", \"Emergency Response Plan\", \n",
    "                            \"Employee Health and Safety Acknowledgment Form\", \"Accident Reporting Procedure\", \n",
    "                            \"Hazardous Materials Handling Policy\"]\n",
    "    }\n",
    "    \n",
    "    # Create documents DataFrame\n",
    "    documents = []\n",
    "    doc_id = 0\n",
    "    data_dir = '../data'  # Change to your documents directory\n",
    "    \n",
    "    for category, doc_names in document_categories.items():\n",
    "        for doc_name in doc_names:\n",
    "            # Construct filename\n",
    "            safe_name = doc_name.replace(' ', '_')\n",
    "            file_path = os.path.join(data_dir, f\"{safe_name}.docx\")\n",
    "            \n",
    "            # Skip if file doesn't exist\n",
    "            if not os.path.exists(file_path):\n",
    "                print(f\"Warning: File not found: {file_path}\")\n",
    "                continue\n",
    "                \n",
    "            # Extract and preprocess text\n",
    "            text = extract_text_from_docx(file_path)\n",
    "            text = preprocess_text(text)\n",
    "            \n",
    "            # Add document info to list\n",
    "            documents.append({\n",
    "                'id': doc_id,\n",
    "                'name': doc_name,\n",
    "                'category': category,\n",
    "                'text': text,\n",
    "                'file_path': file_path\n",
    "            })\n",
    "            doc_id += 1\n",
    "    \n",
    "    docs_df = pd.DataFrame(documents)\n",
    "    print(f\"Created new documents DataFrame with {len(docs_df)} documents\")\n",
    "else:\n",
    "    print(f\"Loaded existing documents DataFrame with {len(docs_df)} documents\")\n",
    "\n",
    "# Display sample documents\n",
    "docs_df[['id', 'name', 'category']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8591dae2",
   "metadata": {},
   "source": [
    "### **Create Chunked Document Segments for Better Search**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704e20df",
   "metadata": {},
   "source": [
    "Long documents are split into manageable chunks:\n",
    "\n",
    "- Documents are divided into smaller, overlapping segments (~200 words each)\n",
    "- Each chunk maintains metadata linking back to its source document\n",
    "- This chunking approach increases search precision by focusing on relevant sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56fe8234",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunking documents: 100%|██████████| 25/25 [00:00<00:00, 3672.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 56 chunks from 25 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chunk_id</th>\n",
       "      <th>doc_id</th>\n",
       "      <th>doc_name</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>General Employment Agreement</td>\n",
       "      <td>Employment Contracts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>General Employment Agreement</td>\n",
       "      <td>Employment Contracts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Non-Compete Clause</td>\n",
       "      <td>Employment Contracts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Non-Compete Clause</td>\n",
       "      <td>Employment Contracts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>Confidentiality Agreement</td>\n",
       "      <td>Employment Contracts</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   chunk_id  doc_id                      doc_name              category\n",
       "0         0       0  General Employment Agreement  Employment Contracts\n",
       "1         1       0  General Employment Agreement  Employment Contracts\n",
       "2         2       1            Non-Compete Clause  Employment Contracts\n",
       "3         3       1            Non-Compete Clause  Employment Contracts\n",
       "4         4       2     Confidentiality Agreement  Employment Contracts"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def chunk_documents(docs_df, chunk_size=200, overlap=50):\n",
    "    \"\"\"\n",
    "    Split documents into smaller overlapping chunks for more precise search results.\n",
    "    \n",
    "    Args:\n",
    "        docs_df: DataFrame with document data\n",
    "        chunk_size: Approximate number of words per chunk\n",
    "        overlap: Number of words overlap between chunks\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with document chunks\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    chunk_id = 0\n",
    "    \n",
    "    for _, doc in tqdm(docs_df.iterrows(), total=len(docs_df), desc=\"Chunking documents\"):\n",
    "        # Split document text into words\n",
    "        words = doc['text'].split()\n",
    "        \n",
    "        # Skip if document is too short\n",
    "        if len(words) < 20:  # Arbitrary minimum size\n",
    "            chunks.append({\n",
    "                'chunk_id': chunk_id,\n",
    "                'doc_id': doc['id'],\n",
    "                'doc_name': doc['name'],\n",
    "                'category': doc['category'],\n",
    "                'text': doc['text'],\n",
    "                'start_idx': 0,\n",
    "                'end_idx': len(words)\n",
    "            })\n",
    "            chunk_id += 1\n",
    "            continue\n",
    "        \n",
    "        # Create overlapping chunks\n",
    "        for i in range(0, len(words), chunk_size - overlap):\n",
    "            # Get chunk words\n",
    "            chunk_words = words[i:i + chunk_size]\n",
    "            \n",
    "            # Skip if chunk is too small (last chunk might be)\n",
    "            if len(chunk_words) < min(20, chunk_size // 4):\n",
    "                continue\n",
    "                \n",
    "            # Join words back into text\n",
    "            chunk_text = ' '.join(chunk_words)\n",
    "            \n",
    "            # Add chunk to list\n",
    "            chunks.append({\n",
    "                'chunk_id': chunk_id,\n",
    "                'doc_id': doc['id'],\n",
    "                'doc_name': doc['name'],\n",
    "                'category': doc['category'],\n",
    "                'text': chunk_text,\n",
    "                'start_idx': i,\n",
    "                'end_idx': i + len(chunk_words)\n",
    "            })\n",
    "            chunk_id += 1\n",
    "    \n",
    "    # Create DataFrame of chunks\n",
    "    chunks_df = pd.DataFrame(chunks)\n",
    "    print(f\"Created {len(chunks_df)} chunks from {len(docs_df)} documents\")\n",
    "    return chunks_df\n",
    "\n",
    "# Create document chunks\n",
    "chunks_df = chunk_documents(docs_df)\n",
    "\n",
    "# Display sample chunks\n",
    "chunks_df[['chunk_id', 'doc_id', 'doc_name', 'category']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ae6b43",
   "metadata": {},
   "source": [
    "### **Generate Embeddings for Document Chunks**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb16cc7e",
   "metadata": {},
   "source": [
    "The system converts text into vector representations:\n",
    "\n",
    "- **Model Selection**: Uses the Sentence Transformers model (`all-MiniLM-L6-v2`)\n",
    "- **Vectorization**: Each text chunk is transformed into a dense embedding vector\n",
    "- These embeddings capture semantic meaning, enabling similarity-based search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b74101f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: all-MiniLM-L6-v2\n",
      "Generating embeddings...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f042581b33cf4476b8cae1dc5d78fec9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings with shape: (56, 384)\n",
      "Embedding dimension: 384\n"
     ]
    }
   ],
   "source": [
    "def generate_embeddings(chunks_df, model_name='all-MiniLM-L6-v2'):\n",
    "    \"\"\"\n",
    "    Generate embeddings for all document chunks using Sentence Transformers.\n",
    "    \n",
    "    Args:\n",
    "        chunks_df: DataFrame with document chunks\n",
    "        model_name: Name of the sentence-transformers model to use\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (chunks_df with embeddings column, embeddings matrix)\n",
    "    \"\"\"\n",
    "    # Load sentence transformer model\n",
    "    print(f\"Loading model: {model_name}\")\n",
    "    model = SentenceTransformer(model_name)\n",
    "    \n",
    "    # Generate embeddings for all chunks\n",
    "    print(\"Generating embeddings...\")\n",
    "    texts = chunks_df['text'].tolist()\n",
    "    embeddings = model.encode(texts, show_progress_bar=True)\n",
    "    \n",
    "    # Convert to float32 for FAISS\n",
    "    embeddings = embeddings.astype(np.float32)\n",
    "    \n",
    "    # Add embeddings to DataFrame\n",
    "    chunks_df['embedding'] = list(embeddings)\n",
    "    \n",
    "    print(f\"Generated embeddings with shape: {embeddings.shape}\")\n",
    "    return chunks_df, embeddings\n",
    "\n",
    "# Generate embeddings for document chunks\n",
    "chunks_df, embeddings_matrix = generate_embeddings(chunks_df)\n",
    "\n",
    "# Check embedding dimensions\n",
    "print(f\"Embedding dimension: {len(chunks_df['embedding'].iloc[0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cfc4374",
   "metadata": {},
   "source": [
    "### **Build and Configure FAISS Index**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4964a210",
   "metadata": {},
   "source": [
    "FAISS (Facebook AI Similarity Search) provides efficient similarity search:\n",
    "\n",
    "- The embedding vectors are normalized and added to a FAISS index\n",
    "- Different index types are available `(flat, IVF, HNSW)` based on dataset size\n",
    "- The index enables fast vector similarity searches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "47a7f3e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built flat FAISS index with 56 vectors\n"
     ]
    }
   ],
   "source": [
    "def build_faiss_index(embeddings, index_type='flat'):\n",
    "    \"\"\"\n",
    "    Build a FAISS index for fast similarity search.\n",
    "    \n",
    "    Args:\n",
    "        embeddings: Matrix of document embeddings\n",
    "        index_type: Type of FAISS index to use ('flat', 'ivf', or 'hnsw')\n",
    "    \n",
    "    Returns:\n",
    "        FAISS index\n",
    "    \"\"\"\n",
    "    # Get embedding dimension\n",
    "    d = embeddings.shape[1]\n",
    "    \n",
    "    # Normalize embeddings for cosine similarity\n",
    "    normalized_embeddings = normalize(embeddings, axis=1, norm='l2')\n",
    "    \n",
    "    if index_type == 'flat':\n",
    "        # Flat index - exact search, most accurate but slowest for large datasets\n",
    "        index = faiss.IndexFlatIP(d)  # Inner product for cosine similarity with normalized vectors\n",
    "    \n",
    "    elif index_type == 'ivf':\n",
    "        # IVF index - approximate search, faster than flat\n",
    "        nlist = min(64, int(embeddings.shape[0] / 10))  # Number of clusters\n",
    "        quantizer = faiss.IndexFlatIP(d)\n",
    "        index = faiss.IndexIVFFlat(quantizer, d, nlist, faiss.METRIC_INNER_PRODUCT)\n",
    "        # Need to train IVF index\n",
    "        index.train(normalized_embeddings)\n",
    "    \n",
    "    elif index_type == 'hnsw':\n",
    "        # HNSW index - approximate search, very fast and memory efficient\n",
    "        index = faiss.IndexHNSWFlat(d, 32, faiss.METRIC_INNER_PRODUCT)  # 32 connections per node\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"Unknown index type: {index_type}\")\n",
    "    \n",
    "    # Add vectors to index\n",
    "    index.add(normalized_embeddings)\n",
    "    \n",
    "    print(f\"Built {index_type} FAISS index with {index.ntotal} vectors\")\n",
    "    return index\n",
    "\n",
    "# Build FAISS index\n",
    "# For small datasets (<10K documents), 'flat' is best\n",
    "# For larger datasets, consider 'ivf' or 'hnsw'\n",
    "index_type = 'flat'  # Choose based on your dataset size\n",
    "faiss_index = build_faiss_index(embeddings_matrix, index_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3722bee",
   "metadata": {},
   "source": [
    "### **Create Search Functions**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4735b363",
   "metadata": {},
   "source": [
    "The search process involves:\n",
    "\n",
    "- Converting a query into the same vector space using the same embedding model\n",
    "- Finding the most similar document chunks in the FAISS index\n",
    "- Deduplicating results to show only the most relevant section from each document\n",
    "- Displaying results with relevance scores and text snippets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e1da244e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 results for query: 'What are the terms of confidentiality in agreements?'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rank</th>\n",
       "      <th>score</th>\n",
       "      <th>doc_name</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.637336</td>\n",
       "      <td>Non-Disclosure Agreement</td>\n",
       "      <td>Commercial Agreements</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.634343</td>\n",
       "      <td>Confidentiality Agreement</td>\n",
       "      <td>Employment Contracts</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rank     score                   doc_name               category\n",
       "0     1  0.637336   Non-Disclosure Agreement  Commercial Agreements\n",
       "1     2  0.634343  Confidentiality Agreement   Employment Contracts"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def search_documents(query, index, chunks_df, model, top_k=5, threshold=0.6):\n",
    "    \"\"\"\n",
    "    Search for documents similar to the query.\n",
    "    \n",
    "    Args:\n",
    "        query: Search query text\n",
    "        index: FAISS index\n",
    "        chunks_df: DataFrame with document chunks and metadata\n",
    "        model: SentenceTransformer model for encoding the query\n",
    "        top_k: Number of results to return\n",
    "        threshold: Minimum similarity score threshold\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with search results\n",
    "    \"\"\"\n",
    "    # Encode query\n",
    "    query_embedding = model.encode([query])[0].astype(np.float32)\n",
    "    \n",
    "    # Normalize query vector for cosine similarity\n",
    "    query_embedding = query_embedding / np.linalg.norm(query_embedding)\n",
    "    \n",
    "    # Search index\n",
    "    distances, indices = index.search(np.array([query_embedding]), top_k)\n",
    "    \n",
    "    # Extract results\n",
    "    results = []\n",
    "    for i, (dist, idx) in enumerate(zip(distances[0], indices[0])):\n",
    "        if idx != -1 and dist >= threshold:  # Skip invalid results and low scores\n",
    "            chunk = chunks_df.iloc[idx]\n",
    "            results.append({\n",
    "                'rank': i + 1,\n",
    "                'score': float(dist),\n",
    "                'chunk_id': int(chunk['chunk_id']),\n",
    "                'doc_id': int(chunk['doc_id']),\n",
    "                'doc_name': chunk['doc_name'],\n",
    "                'category': chunk['category'],\n",
    "                'text_snippet': chunk['text'][:300] + '...' if len(chunk['text']) > 300 else chunk['text']\n",
    "            })\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "def search_with_document_frequency(query, index, chunks_df, model, top_k=5, threshold=0.6, initial_results=20):\n",
    "    \"\"\"\n",
    "    Enhanced search function that accounts for document frequency in search results.\n",
    "    Documents with multiple high-scoring chunks get boosted in ranking.\n",
    "    \n",
    "    Args:\n",
    "        query: Search query text\n",
    "        index: FAISS index\n",
    "        chunks_df: DataFrame with document chunks and metadata\n",
    "        model: SentenceTransformer model\n",
    "        top_k: Number of final results to return\n",
    "        threshold: Minimum similarity score threshold\n",
    "        initial_results: Number of initial chunks to retrieve\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with search results, with documents having multiple hits prioritized\n",
    "    \"\"\"\n",
    "    # Get more initial results to account for document frequency\n",
    "    results = search_documents(query, index, chunks_df, model, top_k=initial_results, threshold=threshold)\n",
    "    \n",
    "    if len(results) == 0:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Calculate document frequency and aggregate scores\n",
    "    doc_scores = {}\n",
    "    for doc_id in results['doc_id'].unique():\n",
    "        # Get all chunks from this document\n",
    "        doc_chunks = results[results['doc_id'] == doc_id]\n",
    "        \n",
    "        # Calculate aggregate score using both score and frequency\n",
    "        # Log frequency prevents documents with many low-relevance chunks from dominating\n",
    "        frequency_boost = 1 + np.log1p(len(doc_chunks))\n",
    "        avg_score = doc_chunks['score'].mean()\n",
    "        max_score = doc_chunks['score'].max()\n",
    "        \n",
    "        # Weighted score - emphasizes both high max score and multiple occurrences\n",
    "        doc_scores[doc_id] = (0.7 * max_score + 0.3 * avg_score) * frequency_boost\n",
    "    \n",
    "    # Rank documents by their overall score\n",
    "    ranked_docs = sorted(doc_scores.items(), key=lambda x: x[1], reverse=True)[:top_k]\n",
    "    \n",
    "    # Prepare final results - include at least one chunk from each top document\n",
    "    final_results = []\n",
    "    used_chunk_ids = set()\n",
    "    \n",
    "    # First, add the best chunk from each top document\n",
    "    for doc_id, doc_score in ranked_docs:\n",
    "        doc_chunks = results[results['doc_id'] == doc_id].sort_values('score', ascending=False)\n",
    "        if len(doc_chunks) > 0:\n",
    "            best_chunk = doc_chunks.iloc[0]\n",
    "            final_results.append({\n",
    "                'doc_id': int(best_chunk['doc_id']),\n",
    "                'chunk_id': int(best_chunk['chunk_id']),\n",
    "                'doc_name': best_chunk['doc_name'],\n",
    "                'category': best_chunk['category'],\n",
    "                'text_snippet': best_chunk['text_snippet'],\n",
    "                'score': float(best_chunk['score']),\n",
    "                'doc_frequency': len(doc_chunks),\n",
    "                'aggregate_score': doc_score,\n",
    "                'rank': 0  # Will be updated later\n",
    "            })\n",
    "            used_chunk_ids.add(best_chunk['chunk_id'])\n",
    "    \n",
    "    # Create final DataFrame\n",
    "    final_df = pd.DataFrame(final_results)\n",
    "    final_df['rank'] = range(1, len(final_df) + 1)\n",
    "    \n",
    "    return final_df\n",
    "\n",
    "def search_and_deduplicate(query, index, chunks_df, model, top_k=5, threshold=0.6):\n",
    "    \"\"\"\n",
    "    Search for documents and deduplicate by document ID.\n",
    "    Returns only the highest-scoring chunk for each document.\n",
    "    \"\"\"\n",
    "    # Get raw search results\n",
    "    results = search_with_document_frequency(query, index, chunks_df, model, top_k=top_k*2, threshold=threshold)\n",
    "    \n",
    "    if len(results) == 0:\n",
    "        return results\n",
    "    \n",
    "    # Deduplicate by doc_id, keeping highest score\n",
    "    results = results.sort_values('score', ascending=False)\n",
    "    deduped_results = results.drop_duplicates(subset=['doc_id']).head(top_k)\n",
    "    \n",
    "    # Reset rank\n",
    "    deduped_results['rank'] = range(1, len(deduped_results) + 1)\n",
    "    \n",
    "    return deduped_results\n",
    "\n",
    "# Load the model used for embeddings\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Test search function with a sample query\n",
    "sample_query = \"What are the terms of confidentiality in agreements?\"\n",
    "search_results = search_and_deduplicate(sample_query, faiss_index, chunks_df, model)\n",
    "\n",
    "# Display search results\n",
    "if len(search_results) > 0:\n",
    "    print(f\"Found {len(search_results)} results for query: '{sample_query}'\")\n",
    "    display(search_results[['rank', 'score', 'doc_name', 'category']])\n",
    "else:\n",
    "    print(f\"No results found for query: '{sample_query}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de40c485",
   "metadata": {},
   "source": [
    "### **Create a Search Evaluation Function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0e5c117c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search Evaluation Metrics:\n",
      "total_queries: 4\n",
      "queries_with_results: 4\n",
      "avg_results_per_query: 1.5\n",
      "avg_top_score: 0.6651738286018372\n",
      "\n",
      "Query: Who owns the intellectual property created by employees?\n",
      "Top document: Intellectual Property Rights (Employment Contracts)\n",
      "Score: 0.6086\n",
      "Found documents: ['Intellectual Property Rights']\n",
      "\n",
      "Query: What is the company's cookie policy?\n",
      "Top document: Cookie Policy (Privacy Policies)\n",
      "Score: 0.6956\n",
      "Found documents: ['Cookie Policy']\n",
      "\n",
      "Query: How should workplace accidents be reported?\n",
      "Top document: Accident Reporting Procedure (Health and Safety)\n",
      "Score: 0.6160\n",
      "Found documents: ['Accident Reporting Procedure', 'Workplace Safety Policy']\n",
      "\n",
      "Query: What are the board of directors' responsibilities?\n",
      "Top document: Board of Directors Responsibilities (Corporate Governance)\n",
      "Score: 0.7405\n",
      "Found documents: ['Board of Directors Responsibilities', 'Bylaws Articles of Association']\n"
     ]
    }
   ],
   "source": [
    "def evaluate_search(test_queries, index, chunks_df, model, ground_truth=None):\n",
    "    \"\"\"\n",
    "    Evaluate search performance on a set of test queries.\n",
    "    \n",
    "    Args:\n",
    "        test_queries: List of query strings or dict mapping queries to expected doc_ids\n",
    "        index: FAISS index\n",
    "        chunks_df: DataFrame with document chunks\n",
    "        model: SentenceTransformer model\n",
    "        ground_truth: Optional dict mapping queries to expected doc_ids\n",
    "    \n",
    "    Returns:\n",
    "        Dict with evaluation metrics\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    # Convert simple list to dict if needed\n",
    "    if isinstance(test_queries, list) and ground_truth is None:\n",
    "        test_queries = {q: None for q in test_queries}\n",
    "    elif ground_truth is not None:\n",
    "        test_queries = ground_truth\n",
    "    \n",
    "    # Run searches\n",
    "    for query, expected_docs in test_queries.items():\n",
    "        search_result = search_and_deduplicate(query, index, chunks_df, model, top_k=5)\n",
    "        \n",
    "        if len(search_result) > 0:\n",
    "            results[query] = {\n",
    "                'found': len(search_result),\n",
    "                'top_doc': search_result.iloc[0]['doc_name'],\n",
    "                'top_category': search_result.iloc[0]['category'],\n",
    "                'top_score': search_result.iloc[0]['score'],\n",
    "                'found_docs': search_result['doc_name'].tolist(),\n",
    "                'expected_found': None\n",
    "            }\n",
    "            \n",
    "            # Check if expected documents were found\n",
    "            if expected_docs is not None:\n",
    "                expected_found = [doc in search_result['doc_id'].tolist() for doc in expected_docs]\n",
    "                results[query]['expected_found'] = sum(expected_found) / len(expected_docs)\n",
    "        else:\n",
    "            results[query] = {\n",
    "                'found': 0,\n",
    "                'top_doc': None,\n",
    "                'top_category': None,\n",
    "                'top_score': None,\n",
    "                'found_docs': [],\n",
    "                'expected_found': 0 if expected_docs else None\n",
    "            }\n",
    "    \n",
    "    # Calculate overall metrics\n",
    "    metrics = {\n",
    "        'total_queries': len(test_queries),\n",
    "        'queries_with_results': sum(1 for r in results.values() if r['found'] > 0),\n",
    "        'avg_results_per_query': np.mean([r['found'] for r in results.values()]),\n",
    "        'avg_top_score': np.mean([r['top_score'] for r in results.values() if r['top_score'] is not None]),\n",
    "    }\n",
    "    \n",
    "    if all(r['expected_found'] is not None for r in results.values()):\n",
    "        metrics['avg_recall'] = np.mean([r['expected_found'] for r in results.values()])\n",
    "    \n",
    "    return results, metrics\n",
    "\n",
    "# Define some test queries\n",
    "test_queries = [\n",
    "    \"Who owns the intellectual property created by employees?\",\n",
    "    \"What is the company's cookie policy?\",\n",
    "    \"How should workplace accidents be reported?\",\n",
    "    \"What are the board of directors' responsibilities?\"\n",
    "    \n",
    "]\n",
    "\n",
    "# Evaluate search performance\n",
    "query_results, metrics = evaluate_search(test_queries, faiss_index, chunks_df, model)\n",
    "\n",
    "# Display evaluation metrics\n",
    "print(\"Search Evaluation Metrics:\")\n",
    "for metric, value in metrics.items():\n",
    "    print(f\"{metric}: {value}\")\n",
    "\n",
    "# Display individual query results\n",
    "for query, result in query_results.items():\n",
    "    print(f\"\\nQuery: {query}\")\n",
    "    print(f\"Top document: {result['top_doc']} ({result['top_category']})\")\n",
    "    print(f\"Score: {result['top_score']:.4f}\")\n",
    "    print(f\"Found documents: {result['found_docs']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd31eab",
   "metadata": {},
   "source": [
    "### **Create a Search Reranking Function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "09320a80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial vs Reranked Results for 'What are the confidentiality terms for vendors?':\n",
      "\n",
      "Initial Top 3:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rank</th>\n",
       "      <th>score</th>\n",
       "      <th>doc_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.66088</td>\n",
       "      <td>Non-Disclosure Agreement</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rank    score                  doc_name\n",
       "0     1  0.66088  Non-Disclosure Agreement"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reranked Top 3:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rank</th>\n",
       "      <th>rerank_score</th>\n",
       "      <th>doc_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>-2.502943</td>\n",
       "      <td>Non-Disclosure Agreement</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rank  rerank_score                  doc_name\n",
       "0     1     -2.502943  Non-Disclosure Agreement"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def rerank_search_results(query, results_df, chunks_df, model, top_k=5):\n",
    "    \"\"\"\n",
    "    Rerank search results using more sophisticated techniques.\n",
    "    This is useful for improving precision when basic vector similarity isn't enough.\n",
    "    \n",
    "    Args:\n",
    "        query: Original search query\n",
    "        results_df: Initial search results\n",
    "        chunks_df: DataFrame with document chunks\n",
    "        model: SentenceTransformer model\n",
    "        top_k: Number of results to return after reranking\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with reranked results\n",
    "    \"\"\"\n",
    "    if len(results_df) == 0:\n",
    "        return results_df\n",
    "    \n",
    "    # Get document chunks for all results\n",
    "    chunks = []\n",
    "    for _, row in results_df.iterrows():\n",
    "        chunk_id = row['chunk_id']\n",
    "        chunk_text = chunks_df.loc[chunks_df['chunk_id'] == chunk_id, 'text'].iloc[0]\n",
    "        chunks.append(chunk_text)\n",
    "    \n",
    "    # Use cross-encoder for reranking if available\n",
    "    try:\n",
    "        from sentence_transformers import CrossEncoder\n",
    "        cross_encoder = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')\n",
    "        \n",
    "        # Create query-document pairs\n",
    "        pairs = [[query, chunk] for chunk in chunks]\n",
    "        \n",
    "        # Score pairs\n",
    "        scores = cross_encoder.predict(pairs)\n",
    "        \n",
    "        # Add new scores to results\n",
    "        results_df['rerank_score'] = scores\n",
    "        \n",
    "        # Sort by new scores\n",
    "        reranked_df = results_df.sort_values('rerank_score', ascending=False).head(top_k)\n",
    "        \n",
    "        # Reset rank\n",
    "        reranked_df['rank'] = range(1, len(reranked_df) + 1)\n",
    "        \n",
    "        return reranked_df\n",
    "    \n",
    "    except ImportError:\n",
    "        print(\"CrossEncoder not available, skipping reranking\")\n",
    "        return results_df.head(top_k)\n",
    "\n",
    "# This is optional - only run if you have the cross-encoder package installed\n",
    "# Try reranking results for a sample query\n",
    "try:\n",
    "    sample_query = \"What are the confidentiality terms for vendors?\"\n",
    "    initial_results = search_documents(sample_query, faiss_index, chunks_df, model, top_k=10)\n",
    "    \n",
    "    if len(initial_results) > 0:\n",
    "        reranked_results = rerank_search_results(sample_query, initial_results, chunks_df, model)\n",
    "        \n",
    "        print(f\"Initial vs Reranked Results for '{sample_query}':\")\n",
    "        print(\"\\nInitial Top 3:\")\n",
    "        display(initial_results[['rank', 'score', 'doc_name']].head(3))\n",
    "        \n",
    "        print(\"\\nReranked Top 3:\")\n",
    "        display(reranked_results[['rank', 'rerank_score', 'doc_name']].head(3))\n",
    "    else:\n",
    "        print(f\"No results found for query: '{sample_query}'\")\n",
    "except Exception as e:\n",
    "    print(f\"Skipping reranking example: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe8553c",
   "metadata": {},
   "source": [
    "### **Create a Document Context Builder Function for RAG**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f6e6ea5",
   "metadata": {},
   "source": [
    "The Retrieval-Augmented Generation (RAG) chatbot:\n",
    "\n",
    "- Takes a user question and converts it to an embedding\n",
    "- Retrieves the most relevant document chunks from the index\n",
    "- Builds a context from these retrieved chunks\n",
    "- Passes this context along with the question to an LLM (GitHub's OpenAI-based service)\n",
    "- The LLM generates an answer grounded in the retrieved documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "43f0e7b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context for query: 'What are my responsibilities regarding workplace safety?'\n",
      "--------------------------------------------------------------------------------\n",
      "Document: Workplace Safety Policy (Category: Health and Safety)\n",
      "ABS Company is committed to providing a safe and healthy work environment for all employees in compliance with applicable occupational health and safety laws, including OSHA regulations (or relevant local standards). Our Workplace Safety Policy outlines the general safety standards, employee responsibilities, and reporting procedures to ensure risk is minimized and everyone’s well-being is safeguarded. General Safety Standards All employees must follow established safety procedures and wear appropriate personal protective equipment (PPE) while performing tasks. Work areas must be kept clean, organized, and free of hazards. Safety signs, emergency exits, and equipment must always be accessible and clearly marked. Machinery and electrical equipment must be operated only by trained and authorized personnel. Regular safety inspections and drills will be conducted to maintain preparedness. Employee Responsibilities Employees sh...\n",
      "--------------------------------------------------------------------------------\n",
      "Source documents: ['Accident Reporting Procedure', 'Hazardous Materials Handling Policy', 'Employee Health and Safety Acknowledgment Form', 'Emergency Response Plan', 'Workplace Safety Policy']\n"
     ]
    }
   ],
   "source": [
    "def build_improved_context(query, chunks_df, index, model, max_chunks=8, max_tokens=2000):\n",
    "    \"\"\"\n",
    "    Build a context from relevant document chunks for RAG applications.\n",
    "    This improved version:\n",
    "    1. Retrieves more initial chunks\n",
    "    2. Considers both individual chunk relevance and document frequency\n",
    "    3. Includes multiple chunks from the same document if they're relevant\n",
    "    \n",
    "    Args:\n",
    "        query: User query\n",
    "        chunks_df: DataFrame with document chunks\n",
    "        index: FAISS index\n",
    "        model: SentenceTransformer model\n",
    "        max_chunks: Maximum number of chunks to include\n",
    "        max_tokens: Approximate maximum number of tokens in context\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (context string, list of source documents)\n",
    "    \"\"\"\n",
    "    # Get a larger initial set of chunks (3x the final desired amount)\n",
    "    # Use search_documents directly to avoid document-level deduplication\n",
    "    initial_results = search_documents(query, index, chunks_df, model, top_k=max_chunks*3, threshold=0.5)\n",
    "    \n",
    "    if len(initial_results) == 0:\n",
    "        return \"No relevant documents found.\", []\n",
    "    \n",
    "    # Calculate document frequency\n",
    "    doc_counts = initial_results['doc_id'].value_counts()\n",
    "    \n",
    "    # Add document frequency information to results\n",
    "    initial_results['doc_frequency'] = initial_results['doc_id'].map(doc_counts)\n",
    "    \n",
    "    # Calculate a combined relevance score that considers:\n",
    "    # 1. Original semantic similarity score (70%)\n",
    "    # 2. Document frequency boost (30%)\n",
    "    initial_results['combined_score'] = (\n",
    "        0.7 * initial_results['score'] + \n",
    "        0.3 * np.log1p(initial_results['doc_frequency']) * initial_results['score']\n",
    "    )\n",
    "    \n",
    "    # Sort by combined score to get the most relevant chunks\n",
    "    # regardless of which document they come from\n",
    "    ranked_chunks = initial_results.sort_values('combined_score', ascending=False)\n",
    "    \n",
    "    # Build context from top chunks\n",
    "    context_parts = []\n",
    "    source_docs = set()  # Track unique source documents\n",
    "    token_count = 0\n",
    "    word_to_token_ratio = 0.75\n",
    "    \n",
    "    # Take chunks based on combined score until we hit our limits\n",
    "    for _, row in ranked_chunks.iterrows():\n",
    "        if len(context_parts) >= max_chunks:\n",
    "            break\n",
    "            \n",
    "        chunk_id = row['chunk_id']\n",
    "        doc_id = row['doc_id']\n",
    "        doc_name = row['doc_name']\n",
    "        category = row['category']\n",
    "        \n",
    "        # Get full chunk text\n",
    "        chunk_text = chunks_df.loc[chunks_df['chunk_id'] == chunk_id, 'text'].iloc[0]\n",
    "        \n",
    "        # Estimate token count\n",
    "        words = len(chunk_text.split())\n",
    "        estimated_tokens = int(words * word_to_token_ratio)\n",
    "        \n",
    "        # Skip if adding this would exceed token limit\n",
    "        if token_count + estimated_tokens > max_tokens:\n",
    "            continue\n",
    "            \n",
    "        # Add chunk to context\n",
    "        context_parts.append(f\"Document: {doc_name} (Category: {category})\\n{chunk_text}\\n\\n\")\n",
    "        token_count += estimated_tokens\n",
    "        \n",
    "        # Track source documents for attribution\n",
    "        source_docs.add(doc_name)\n",
    "    \n",
    "    # Combine all parts\n",
    "    full_context = \"\".join(context_parts)\n",
    "    \n",
    "    return full_context, list(source_docs)\n",
    "\n",
    "# Test context builder with a sample query\n",
    "sample_query = \"What are my responsibilities regarding workplace safety?\"\n",
    "context , source_docs = build_improved_context(sample_query, chunks_df, faiss_index, model)\n",
    "\n",
    "print(f\"Context for query: '{sample_query}'\")\n",
    "print(\"-\" * 80)\n",
    "print(context[:1000] + \"...\" if len(context) > 1000 else context)\n",
    "print(\"-\" * 80)\n",
    "print(f\"Source documents: {source_docs}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4f7517",
   "metadata": {},
   "source": [
    "### **Create a Simple Search Interface Function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d2685d33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for: 'What are the intellectual property rights for employee created work?'\n",
      "--------------------------------------------------------------------------------\n",
      "Found 1 relevant documents:\n",
      "1. Intellectual Property Rights (Employment Contracts) - Score: 0.6344\n",
      "   Snippet: This **INTELLECTUAL** **PROPERTY** **RIGHTS** Clause (“Clause”) forms an integral part of the employment agreement between ABS Company (“Employer”) and Michael Green (“**EMPLOYEE**”), collectively referred to as the “Parties.” 1. Ownership of **INTELLECTUAL** **PROPERTY** Michael Green acknowledges and agrees that all inve...\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Searching for: 'How do we handle cookie consent for website visitors?'\n",
      "--------------------------------------------------------------------------------\n",
      "Found 1 relevant documents:\n",
      "1. Cookie Policy (Privacy Policies) - Score: 0.6395\n",
      "   Snippet: This **COOKIE** Policy explains how ABS Company (“we”, “us”, or “our”) uses **COOKIE**s and similar tracking technologies on our **WEBSITE** to enhance user experience, analyze site usage, and provide personalized content. What Are **COOKIE**s? **COOKIE**s are small text files placed on your device (computer, smartphon...\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Searching for: 'What steps should be taken after a workplace accident?'\n",
      "--------------------------------------------------------------------------------\n",
      "No results found.\n",
      "\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def search_interface(query, chunks_df, index, model, show_snippets=True):\n",
    "    \"\"\"\n",
    "    Simple search interface function for displaying search results.\n",
    "    \n",
    "    Args:\n",
    "        query: Search query\n",
    "        chunks_df: DataFrame with document chunks\n",
    "        index: FAISS index\n",
    "        model: SentenceTransformer model\n",
    "        show_snippets: Whether to show text snippets\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with search results\n",
    "    \"\"\"\n",
    "    print(f\"Searching for: '{query}'\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    # Search for documents\n",
    "    results = search_and_deduplicate(query, index, chunks_df, model)\n",
    "    \n",
    "    if len(results) == 0:\n",
    "        print(\"No results found.\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    print(f\"Found {len(results)} relevant documents:\")\n",
    "    \n",
    "    # Display results\n",
    "    for _, row in results.iterrows():\n",
    "        print(f\"{row['rank']}. {row['doc_name']} ({row['category']}) - Score: {row['score']:.4f}\")\n",
    "        \n",
    "        if show_snippets:\n",
    "            snippet = row['text_snippet']\n",
    "            # Highlight query terms (simple approach)\n",
    "            for term in query.lower().split():\n",
    "                if len(term) > 3:  # Only highlight meaningful terms\n",
    "                    pattern = re.compile(re.escape(term), re.IGNORECASE)\n",
    "                    snippet = pattern.sub(f\"**{term.upper()}**\", snippet)\n",
    "            \n",
    "            print(f\"   Snippet: {snippet}\")\n",
    "            print()\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test search interface with a few queries\n",
    "test_queries = [\n",
    "    \"What are the intellectual property rights for employee created work?\",\n",
    "    \"How do we handle cookie consent for website visitors?\",\n",
    "    \"What steps should be taken after a workplace accident?\",\n",
    "]\n",
    "\n",
    "for query in test_queries:\n",
    "    results = search_interface(query, chunks_df, faiss_index, model)\n",
    "    print(\"\\n\" + \"=\" * 80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c91cbb",
   "metadata": {},
   "source": [
    "### **Save and Load the Search Engine**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "884d09d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search engine saved to ../search_engine\n",
      "Loaded search engine with 56 chunks and 56 vectors\n",
      "Successfully loaded search engine!\n"
     ]
    }
   ],
   "source": [
    "def save_search_engine(index, chunks_df, output_dir='../search_engine'):\n",
    "    \"\"\"\n",
    "    Save the search engine components to disk.\n",
    "    \n",
    "    Args:\n",
    "        index: FAISS index\n",
    "        chunks_df: DataFrame with document chunks\n",
    "        output_dir: Directory to save files\n",
    "    \"\"\"\n",
    "    import os\n",
    "    \n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Save FAISS index\n",
    "    faiss.write_index(index, os.path.join(output_dir, 'document_index.faiss'))\n",
    "    \n",
    "    # Save chunks DataFrame (without embeddings to save space)\n",
    "    save_df = chunks_df.copy()\n",
    "    if 'embedding' in save_df.columns:\n",
    "        save_df.drop('embedding', axis=1, inplace=True)\n",
    "    save_df.to_pickle(os.path.join(output_dir, 'document_chunks.pkl'))\n",
    "    \n",
    "    # Save embeddings separately\n",
    "    embeddings = np.vstack(chunks_df['embedding'].to_numpy())\n",
    "    np.save(os.path.join(output_dir, 'embeddings.npy'), embeddings)\n",
    "    \n",
    "    print(f\"Search engine saved to {output_dir}\")\n",
    "\n",
    "def load_search_engine(input_dir='../search_engine'):\n",
    "    \"\"\"\n",
    "    Load the search engine components from disk.\n",
    "    \n",
    "    Args:\n",
    "        input_dir: Directory with saved files\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (FAISS index, chunks DataFrame)\n",
    "    \"\"\"\n",
    "    import os\n",
    "    \n",
    "    # Check if directory exists\n",
    "    if not os.path.exists(input_dir):\n",
    "        raise FileNotFoundError(f\"Directory not found: {input_dir}\")\n",
    "    \n",
    "    # Load FAISS index\n",
    "    index = faiss.read_index(os.path.join(input_dir, 'document_index.faiss'))\n",
    "    \n",
    "    # Load chunks DataFrame\n",
    "    chunks_df = pd.read_pickle(os.path.join(input_dir, 'document_chunks.pkl'))\n",
    "    \n",
    "    # Load embeddings\n",
    "    embeddings = np.load(os.path.join(input_dir, 'embeddings.npy'))\n",
    "    \n",
    "    # Add embeddings back to DataFrame\n",
    "    chunks_df['embedding'] = list(embeddings)\n",
    "    \n",
    "    print(f\"Loaded search engine with {len(chunks_df)} chunks and {index.ntotal} vectors\")\n",
    "    return index, chunks_df\n",
    "\n",
    "# Save the search engine\n",
    "save_search_engine(faiss_index, chunks_df)\n",
    "\n",
    "# Test loading the search engine (optional)\n",
    "try:\n",
    "    loaded_index, loaded_chunks_df = load_search_engine()\n",
    "    print(\"Successfully loaded search engine!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading search engine: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a669911f",
   "metadata": {},
   "source": [
    "### **Integration with LLM for RAG Chatbot (Using OpenAI)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b585dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def improved_rag_chatbot(query, chunks_df, index, model, github_token=None):\n",
    "    \"\"\"\n",
    "    Enhanced RAG-based chatbot that:\n",
    "    1. Uses improved context builder to get most relevant chunks\n",
    "    2. Provides source attribution for answers\n",
    "    3. Returns both the answer and sources used\n",
    "    \n",
    "    Args:\n",
    "        query: User query\n",
    "        chunks_df: DataFrame with document chunks\n",
    "        index: FAISS index\n",
    "        model: SentenceTransformer model\n",
    "        github_token: GitHub token (if None, will use environment variable)\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with keys: 'answer', 'sources'\n",
    "    \"\"\"\n",
    "    import os\n",
    "    \n",
    "    # Get GitHub token from environment variable if not provided\n",
    "    if github_token is None:\n",
    "        github_token = os.environ.get('GITHUB_TOKEN')\n",
    "    \n",
    "    if github_token is None:\n",
    "        return {\n",
    "            \"answer\": \"GitHub token not provided. Please set GITHUB_TOKEN environment variable or pass token as parameter.\",\n",
    "            \"sources\": []\n",
    "        }\n",
    "    \n",
    "    try:\n",
    "        from azure.ai.inference import ChatCompletionsClient\n",
    "        from azure.ai.inference.models import SystemMessage, UserMessage\n",
    "        from azure.core.credentials import AzureKeyCredential\n",
    "        \n",
    "        # Build context using improved method\n",
    "        context, source_docs = build_improved_context(query, chunks_df, index, model)\n",
    "        \n",
    "        # Create Azure AI Inference client\n",
    "        client = ChatCompletionsClient(\n",
    "            endpoint=\"https://models.github.ai/inference\",\n",
    "            credential=AzureKeyCredential(github_token),\n",
    "        )\n",
    "        \n",
    "        # Enhanced system prompt that encourages citation\n",
    "        system_prompt = f\"\"\"You are a legal assistant for ABS Company. Answer the user's question based ONLY on the context provided below.\n",
    "        If the answer is not in the context, say \"I don't have enough information to answer this question.\" Do not make up information.\n",
    "        \n",
    "        When answering, refer to specific documents by name when you use information from them.\n",
    "        \n",
    "        Context:\n",
    "        {context}\"\"\"\n",
    "        \n",
    "        # Call GitHub's LLM API\n",
    "        response = client.complete(\n",
    "            messages=[\n",
    "                SystemMessage(system_prompt),\n",
    "                UserMessage(query)\n",
    "            ],\n",
    "            model=\"openai/gpt-4o-mini\",  # Using GitHub's model\n",
    "            temperature=0.3,\n",
    "            max_tokens=500,\n",
    "            top_p=1\n",
    "        )\n",
    "        \n",
    "        answer = response.choices[0].message.content\n",
    "        \n",
    "        # Return both the answer and the sources used\n",
    "        return {\n",
    "            \"answer\": answer,\n",
    "            \"sources\": source_docs\n",
    "        }\n",
    "    \n",
    "    except ImportError:\n",
    "        return {\n",
    "            \"answer\": \"Azure AI Inference package not installed. Use 'pip install azure-ai-inference' to install it.\",\n",
    "            \"sources\": []\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"answer\": f\"Error using GitHub LLM API: {str(e)}\",\n",
    "            \"sources\": []\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31807534",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Improved RAG Implementation\n",
      "\n",
      "Question: What are the intellectual property rights for employee created work?\n",
      "--------------------------------------------------------------------------------\n",
      "Answer: According to the \"Intellectual Property Rights\" document, all inventions, discoveries, developments, improvements, processes, designs, works of authorship, trade secrets, patents, copyrights, trademarks, and any other intellectual property (IP) conceived, created, developed, or reduced to practice by Michael Green during his employment with ABS Company, and within the scope of his work or using ABS Company resources, shall be the exclusive property of ABS Company. \n",
      "\n",
      "Michael Green is required to assign all rights to this IP to ABS Company immediately upon its creation. He must also disclose any IP created during his employment that relates to ABS Company’s business and assist the company in securing and enforcing IP rights. However, IP created entirely on his own time, without using ABS resources, and unrelated to the Company’s business, shall remain his property, provided he meets disclosure obligations and there is no conflict with Company interests.\n",
      "\n",
      "Sources (1):\n",
      "1. Intellectual Property Rights\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Question: How do we handle cookie consent for website visitors?\n",
      "--------------------------------------------------------------------------------\n",
      "Answer: According to the Cookie Policy, upon a visitor's first visit to the ABS Company website, a cookie consent banner will appear to inform them and obtain their consent. The banner states: “ABS Company uses cookies to improve your experience and analyze site traffic. By clicking ‘Accept All’, you consent to our use of cookies.” Visitors have the option to accept all cookies, reject non-essential cookies (such as marketing or analytics), or adjust their cookie preferences through their browser settings or the cookie banner options.\n",
      "\n",
      "Sources (1):\n",
      "1. Cookie Policy\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Question: Who owns the intellectual property for work I create during my employment?\n",
      "--------------------------------------------------------------------------------\n",
      "Answer: According to the Intellectual Property Rights Clause in the employment agreement between ABS Company and Michael Green, all intellectual property created by Michael Green during his employment, within the scope of his work or using ABS Company resources, shall be the exclusive property of ABS Company.\n",
      "\n",
      "Sources (1):\n",
      "1. Intellectual Property Rights\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Set your GitHub token in environment variables\n",
    "import os\n",
    "# Use your GitHub personal access token here\n",
    "os.environ['GITHUB_TOKEN'] = ' '  # GitHub token\n",
    "\n",
    "def demonstrate_improved_rag():\n",
    "    test_questions = [\n",
    "        \"What are the intellectual property rights for employee created work?\",\n",
    "        \"How do we handle cookie consent for website visitors?\",\n",
    "        \"Who owns the intellectual property for work I create during my employment?\"\n",
    "    ]\n",
    "    \n",
    "    print(\"Testing Improved RAG Implementation\\n\")\n",
    "    \n",
    "    for question in test_questions:\n",
    "        print(f\"Question: {question}\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        # Get response from improved RAG chatbot\n",
    "        response = improved_rag_chatbot(question, chunks_df, faiss_index, model)\n",
    "        \n",
    "        # Display the answer\n",
    "        print(f\"Answer: {response['answer']}\\n\")\n",
    "        \n",
    "        # Display source documents used\n",
    "        print(f\"Sources ({len(response['sources'])}):\")\n",
    "        for i, source in enumerate(response['sources'], 1):\n",
    "            print(f\"{i}. {source}\")\n",
    "        \n",
    "        print(\"\\n\" + \"-\" * 80 + \"\\n\")\n",
    "\n",
    "# Run the demonstration\n",
    "demonstrate_improved_rag()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201d1825",
   "metadata": {},
   "source": [
    "### ***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
