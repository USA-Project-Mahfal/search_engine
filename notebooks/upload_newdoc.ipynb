{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee5628b0",
   "metadata": {},
   "source": [
    "# Implementing Document Upload for Your Search Engine in a New Notebook\n",
    "\n",
    "\n",
    "\n",
    "Create a complete implementation for you that does the following:\n",
    "1. Imports your existing search engine\n",
    "2. Adds functionality to upload and process new DOCX files\n",
    "3. Updates the index with new documents\n",
    "4. Provides search capabilities on the expanded document set\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d33878",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import faiss\n",
    "import re\n",
    "import docx\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import logging\n",
    "import json\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger('document_upload_system')\n",
    "\n",
    "# Add the path to your original implementation if needed\n",
    "# sys.path.append('../path/to/original/implementation')\n",
    "\n",
    "# Step 1: Load the existing search engine\n",
    "def load_search_engine(input_dir='../optimized_search_engine'):\n",
    "    \"\"\"\n",
    "    Load a complete search engine from disk.\n",
    "    \n",
    "    Args:\n",
    "        input_dir: Directory containing the search engine\n",
    "        \n",
    "    Returns:\n",
    "        Initialized SemanticSearchEngine instance\n",
    "    \"\"\"\n",
    "    if not os.path.exists(input_dir):\n",
    "        logger.error(f\"Directory not found: {input_dir}\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        # Load model name and initialize\n",
    "        with open(os.path.join(input_dir, 'model_name.txt'), 'r') as f:\n",
    "            model_name = f.read().strip()\n",
    "        \n",
    "        model = SentenceTransformer(model_name)\n",
    "        \n",
    "        # Load chunks\n",
    "        chunks_path = os.path.join(input_dir, 'chunks_df.pkl')\n",
    "        chunks_df = pd.read_pickle(chunks_path)\n",
    "        \n",
    "        # Load embeddings if they exist\n",
    "        embeddings_path = os.path.join(input_dir, 'embeddings.npy')\n",
    "        if os.path.exists(embeddings_path):\n",
    "            embeddings = np.load(embeddings_path)\n",
    "            # Add embeddings back to DataFrame\n",
    "            chunks_df['embedding'] = list(embeddings)\n",
    "        \n",
    "        # Load FAISS index\n",
    "        index_path = os.path.join(input_dir, 'optimized_index.faiss')\n",
    "        index = faiss.read_index(index_path)\n",
    "        \n",
    "        # Creating a dictionary with all components\n",
    "        engine_components = {\n",
    "            'chunks_df': chunks_df,\n",
    "            'index': index,\n",
    "            'model': model,\n",
    "            'model_name': model_name,\n",
    "            'engine_dir': input_dir\n",
    "        }\n",
    "        \n",
    "        # Import SemanticSearchEngine class\n",
    "        # We'll recreate a simplified version for this example\n",
    "        class SemanticSearchEngine:\n",
    "            def __init__(self, chunks_df=None, index=None, model=None):\n",
    "                self.chunks_df = chunks_df\n",
    "                self.index = index\n",
    "                self.model = model\n",
    "                self.metadata = {}\n",
    "            \n",
    "            def search(self, query, method=None, params=None, categories=None, auto_select=True):\n",
    "                logger.info(f\"Searching for: {query}\")\n",
    "                # This is a placeholder - actual implementation would use the existing code\n",
    "                # In a real implementation, you'd import the search functionality\n",
    "                \n",
    "                # Here we'd call: return search_documents(query, self.index, self.chunks_df, self.model)\n",
    "                return pd.DataFrame({\"message\": [\"Search functionality would be implemented here\"]})\n",
    "        \n",
    "        # Create search engine\n",
    "        engine = SemanticSearchEngine(\n",
    "            chunks_df=chunks_df,\n",
    "            index=index,\n",
    "            model=model\n",
    "        )\n",
    "        \n",
    "        logger.info(f\"Search engine loaded with {len(chunks_df)} chunks and {index.ntotal} vectors\")\n",
    "        return engine, engine_components\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading search engine: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# Step 2: Document processing functions\n",
    "def extract_text_from_docx(file_path):\n",
    "    \"\"\"Extract text content from a .docx file.\"\"\"\n",
    "    try:\n",
    "        doc = docx.Document(file_path)\n",
    "        full_text = []\n",
    "        for para in doc.paragraphs:\n",
    "            if para.text.strip():  # Skip empty paragraphs\n",
    "                full_text.append(para.text)\n",
    "        return '\\n'.join(full_text)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing {file_path}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Clean and preprocess text.\"\"\"\n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "# Step 3: Document chunking (semantic chunking from original implementation)\n",
    "def semantic_chunking(doc_df, min_chunk_size=100, max_chunk_size=300):\n",
    "    \"\"\"\n",
    "    Split documents into chunks based on semantic boundaries like paragraphs and sections.\n",
    "    \n",
    "    Args:\n",
    "        doc_df: DataFrame with document data\n",
    "        min_chunk_size: Minimum words per chunk\n",
    "        max_chunk_size: Maximum words per chunk\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with document chunks\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    chunk_id_start = 0\n",
    "    \n",
    "    # Get the highest existing chunk_id to avoid duplicates\n",
    "    if 'existing_chunks_df' in globals() and existing_chunks_df is not None:\n",
    "        if len(existing_chunks_df) > 0:\n",
    "            chunk_id_start = existing_chunks_df['chunk_id'].max() + 1\n",
    "    \n",
    "    # Patterns for section boundaries\n",
    "    section_patterns = [\n",
    "        r'\\n#{1,3}\\s+.+\\n',  # Markdown headers\n",
    "        r'\\n\\d+\\.\\s+[A-Z]',  # Numbered sections starting with capital letter\n",
    "        r'\\n[A-Z][A-Z\\s]+\\n',  # All caps section titles\n",
    "        r'\\n[A-Z][a-z]+\\s+\\d+[\\\\.:]\\\\s+',  # Article X: style headers\n",
    "        r'\\n\\s*SECTION\\s+\\d+',  # SECTION X style headers\n",
    "    ]\n",
    "    \n",
    "    compiled_patterns = [re.compile(pattern) for pattern in section_patterns]\n",
    "    \n",
    "    for _, doc in tqdm(doc_df.iterrows(), total=len(doc_df), desc=\"Creating semantic chunks\"):\n",
    "        text = doc['text']\n",
    "        \n",
    "        # Find all potential section boundaries\n",
    "        boundaries = []\n",
    "        for pattern in compiled_patterns:\n",
    "            for match in pattern.finditer(text):\n",
    "                boundaries.append(match.start())\n",
    "        \n",
    "        # Sort boundaries and add start/end of document\n",
    "        boundaries = sorted(list(set([0] + boundaries + [len(text)])))\n",
    "        \n",
    "        # Determine total chunks for position scoring\n",
    "        total_chunks = len(boundaries) - 1\n",
    "        \n",
    "        # Create chunks based on boundaries\n",
    "        chunk_id = chunk_id_start\n",
    "        for i in range(len(boundaries) - 1):\n",
    "            chunk_text = text[boundaries[i]:boundaries[i+1]].strip()\n",
    "            \n",
    "            # Skip empty chunks\n",
    "            if not chunk_text:\n",
    "                continue\n",
    "                \n",
    "            # Determine position information\n",
    "            if i < total_chunks / 3:\n",
    "                position = \"beginning\"\n",
    "                position_score = 0.9  # Prefer beginning chunks slightly\n",
    "            elif i > 2 * total_chunks / 3:\n",
    "                position = \"end\"\n",
    "                position_score = 0.7\n",
    "            else:\n",
    "                position = \"middle\"\n",
    "                position_score = 0.8\n",
    "                \n",
    "            # Check if chunk is too small or too large\n",
    "            words = chunk_text.split()\n",
    "            if len(words) < min_chunk_size:\n",
    "                # If too small, combine with next chunk if possible\n",
    "                if i < len(boundaries) - 2:\n",
    "                    continue\n",
    "            \n",
    "            # If too large, split into smaller chunks\n",
    "            if len(words) > max_chunk_size:\n",
    "                sub_chunks = [' '.join(words[j:j+max_chunk_size]) \n",
    "                            for j in range(0, len(words), max_chunk_size)]\n",
    "                \n",
    "                # Handle subchunks with appropriate position scoring\n",
    "                for sub_idx, sub_chunk in enumerate(sub_chunks):\n",
    "                    # Adjust position for subchunks\n",
    "                    if sub_idx == 0:\n",
    "                        sub_position = position\n",
    "                        sub_position_score = position_score\n",
    "                    elif sub_idx == len(sub_chunks) - 1:\n",
    "                        sub_position = position\n",
    "                        sub_position_score = position_score * 0.9  # Slightly lower\n",
    "                    else:\n",
    "                        sub_position = \"middle\"\n",
    "                        sub_position_score = position_score * 0.8  # Lower for middle subchunks\n",
    "                    \n",
    "                    chunks.append({\n",
    "                        'chunk_id': chunk_id,\n",
    "                        'doc_id': doc['id'],\n",
    "                        'doc_name': doc['name'],\n",
    "                        'category': doc['category'],\n",
    "                        'text': sub_chunk,\n",
    "                        'chunk_method': 'semantic_split',\n",
    "                        'start_idx': boundaries[i],\n",
    "                        'end_idx': boundaries[i+1],\n",
    "                        'document_position': sub_position,\n",
    "                        'position_score': sub_position_score\n",
    "                    })\n",
    "                    chunk_id += 1\n",
    "            else:\n",
    "                # Add as a single chunk\n",
    "                chunks.append({\n",
    "                    'chunk_id': chunk_id,\n",
    "                    'doc_id': doc['id'],\n",
    "                    'doc_name': doc['name'],\n",
    "                    'category': doc['category'],\n",
    "                    'text': chunk_text,\n",
    "                    'chunk_method': 'semantic',\n",
    "                    'start_idx': boundaries[i],\n",
    "                    'end_idx': boundaries[i+1],\n",
    "                    'document_position': position,\n",
    "                    'position_score': position_score\n",
    "                })\n",
    "                chunk_id += 1\n",
    "    \n",
    "    # Create DataFrame of chunks\n",
    "    new_chunks_df = pd.DataFrame(chunks)\n",
    "    logger.info(f\"Created {len(new_chunks_df)} semantic chunks from {len(doc_df)} documents\")\n",
    "    return new_chunks_df\n",
    "\n",
    "# Step 4: Function to upload and process new documents\n",
    "def upload_documents(file_paths, categories, engine_components):\n",
    "    \"\"\"\n",
    "    Upload, process and index new documents.\n",
    "    \n",
    "    Args:\n",
    "        file_paths: List of paths to new document files\n",
    "        categories: List of categories for each document\n",
    "        engine_components: Dictionary with engine components\n",
    "        \n",
    "    Returns:\n",
    "        Updated engine components\n",
    "    \"\"\"\n",
    "    # Check that file_paths and categories have same length\n",
    "    if len(file_paths) != len(categories):\n",
    "        logger.error(\"The number of file paths must match the number of categories\")\n",
    "        return engine_components\n",
    "    \n",
    "    # Extract components\n",
    "    chunks_df = engine_components['chunks_df']\n",
    "    index = engine_components['index']\n",
    "    model = engine_components['model']\n",
    "    \n",
    "    # Get highest doc_id to avoid duplicates\n",
    "    next_doc_id = 0\n",
    "    if len(chunks_df) > 0:\n",
    "        next_doc_id = chunks_df['doc_id'].max() + 1\n",
    "    \n",
    "    # Process new documents\n",
    "    new_docs = []\n",
    "    for i, file_path in enumerate(file_paths):\n",
    "        if not os.path.exists(file_path):\n",
    "            logger.warning(f\"File not found: {file_path}\")\n",
    "            continue\n",
    "            \n",
    "        # Extract document name from file path\n",
    "        doc_name = os.path.splitext(os.path.basename(file_path))[0].replace('_', ' ')\n",
    "        \n",
    "        # Extract and preprocess text\n",
    "        text = extract_text_from_docx(file_path)\n",
    "        text = preprocess_text(text)\n",
    "        \n",
    "        if not text:\n",
    "            logger.warning(f\"No text content found in {file_path}\")\n",
    "            continue\n",
    "        \n",
    "        # Add document to list\n",
    "        new_docs.append({\n",
    "            'id': next_doc_id,\n",
    "            'name': doc_name,\n",
    "            'category': categories[i],\n",
    "            'text': text,\n",
    "            'file_path': file_path\n",
    "        })\n",
    "        next_doc_id += 1\n",
    "    \n",
    "    # Create DataFrame for new documents\n",
    "    new_docs_df = pd.DataFrame(new_docs)\n",
    "    logger.info(f\"Processed {len(new_docs_df)} new documents\")\n",
    "    \n",
    "    if len(new_docs_df) == 0:\n",
    "        logger.warning(\"No valid documents to add\")\n",
    "        return engine_components\n",
    "    \n",
    "    # Create chunks for new documents\n",
    "    global existing_chunks_df\n",
    "    existing_chunks_df = chunks_df\n",
    "    new_chunks_df = semantic_chunking(new_docs_df)\n",
    "    \n",
    "    # Generate embeddings for new chunks\n",
    "    logger.info(\"Generating embeddings for new chunks...\")\n",
    "    new_texts = new_chunks_df['text'].tolist()\n",
    "    \n",
    "    # Batch processing for embeddings\n",
    "    batch_size = 32\n",
    "    new_embeddings = np.zeros((len(new_texts), model.get_sentence_embedding_dimension()), dtype=np.float32)\n",
    "    \n",
    "    for i in tqdm(range(0, len(new_texts), batch_size), desc=\"Batch encoding\"):\n",
    "        batch_texts = new_texts[i:i+batch_size]\n",
    "        batch_embeddings = model.encode(batch_texts, show_progress_bar=False)\n",
    "        new_embeddings[i:i+len(batch_texts)] = batch_embeddings\n",
    "    \n",
    "    # Add embeddings to DataFrame\n",
    "    new_chunks_df['embedding'] = list(new_embeddings)\n",
    "    \n",
    "    # Update the index with new embeddings\n",
    "    normalized_embeddings = new_embeddings.copy()\n",
    "    faiss.normalize_L2(normalized_embeddings)\n",
    "    \n",
    "    # Add vectors to index\n",
    "    index.add(normalized_embeddings)\n",
    "    logger.info(f\"Added {len(new_embeddings)} vectors to the index\")\n",
    "    \n",
    "    # Combine new chunks with existing chunks\n",
    "    updated_chunks_df = pd.concat([chunks_df, new_chunks_df], ignore_index=False)\n",
    "    logger.info(f\"Updated chunks DataFrame now has {len(updated_chunks_df)} chunks\")\n",
    "    \n",
    "    # Update engine components\n",
    "    engine_components['chunks_df'] = updated_chunks_df\n",
    "    engine_components['index'] = index\n",
    "    \n",
    "    return engine_components\n",
    "\n",
    "# Step 5: Function to save the updated search engine\n",
    "def save_updated_engine(engine_components, output_dir=None):\n",
    "    \"\"\"\n",
    "    Save the updated search engine to disk.\n",
    "    \n",
    "    Args:\n",
    "        engine_components: Dictionary with engine components\n",
    "        output_dir: Directory to save the search engine (defaults to original location)\n",
    "    \"\"\"\n",
    "    if output_dir is None:\n",
    "        output_dir = engine_components.get('engine_dir', '../updated_search_engine')\n",
    "    \n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Extract components\n",
    "    chunks_df = engine_components['chunks_df']\n",
    "    index = engine_components['index']\n",
    "    model_name = engine_components['model_name']\n",
    "    \n",
    "    # Save FAISS index\n",
    "    index_path = os.path.join(output_dir, 'optimized_index.faiss')\n",
    "    faiss.write_index(index, index_path)\n",
    "    \n",
    "    # Save chunks DataFrame (without embeddings)\n",
    "    chunks_path = os.path.join(output_dir, 'chunks_df.pkl')\n",
    "    save_df = chunks_df.copy()\n",
    "    \n",
    "    if 'embedding' in save_df.columns:\n",
    "        # Save embeddings separately\n",
    "        embeddings = np.vstack(save_df['embedding'].values)\n",
    "        embeddings_path = os.path.join(output_dir, 'embeddings.npy')\n",
    "        np.save(embeddings_path, embeddings)\n",
    "        # Remove embeddings from DataFrame to save space\n",
    "        save_df = save_df.drop('embedding', axis=1)\n",
    "    \n",
    "    save_df.to_pickle(chunks_path)\n",
    "    \n",
    "    # Save model name\n",
    "    with open(os.path.join(output_dir, 'model_name.txt'), 'w') as f:\n",
    "        f.write(model_name)\n",
    "    \n",
    "    # Save configuration\n",
    "    config = {\n",
    "        'saved_date': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        'chunk_count': len(chunks_df),\n",
    "        'document_count': len(chunks_df['doc_id'].unique()),\n",
    "        'categories': chunks_df['category'].unique().tolist(),\n",
    "        'index_size': index.ntotal,\n",
    "        'embedding_dim': embeddings.shape[1] if 'embeddings' in locals() else None,\n",
    "    }\n",
    "    \n",
    "    with open(os.path.join(output_dir, 'search_engine_config.json'), 'w') as f:\n",
    "        json.dump(config, f, indent=2)\n",
    "    \n",
    "    logger.info(f\"Updated search engine saved to {output_dir}\")\n",
    "    return output_dir\n",
    "\n",
    "# Step 6: Create a SemanticSearchEngine instance with document upload capabilities\n",
    "class UpgradedSearchEngine:\n",
    "    \"\"\"\n",
    "    Enhanced search engine with document upload capabilities.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, engine_path='../optimized_search_engine'):\n",
    "        \"\"\"\n",
    "        Initialize by loading an existing search engine.\n",
    "        \n",
    "        Args:\n",
    "            engine_path: Path to the existing search engine\n",
    "        \"\"\"\n",
    "        self.engine, self.components = load_search_engine(engine_path)\n",
    "        \n",
    "        if self.engine is None:\n",
    "            logger.error(\"Failed to load search engine\")\n",
    "        else:\n",
    "            logger.info(\"Upgraded search engine initialized successfully\")\n",
    "    \n",
    "    def search(self, query, method=None, params=None, categories=None, auto_select=True):\n",
    "        \"\"\"\n",
    "        Search using the underlying search engine.\n",
    "        \"\"\"\n",
    "        if self.engine is None:\n",
    "            logger.error(\"Search engine not initialized\")\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        return self.engine.search(\n",
    "            query=query, \n",
    "            method=method,\n",
    "            params=params,\n",
    "            categories=categories,\n",
    "            auto_select=auto_select\n",
    "        )\n",
    "    \n",
    "    def upload_new_documents(self, file_paths, categories):\n",
    "        \"\"\"\n",
    "        Upload and process new documents.\n",
    "        \n",
    "        Args:\n",
    "            file_paths: List of paths to document files\n",
    "            categories: List of categories for the documents\n",
    "            \n",
    "        Returns:\n",
    "            True if successful, False otherwise\n",
    "        \"\"\"\n",
    "        if self.engine is None or self.components is None:\n",
    "            logger.error(\"Search engine not initialized\")\n",
    "            return False\n",
    "        \n",
    "        try:\n",
    "            # Process and index new documents\n",
    "            updated_components = upload_documents(file_paths, categories, self.components)\n",
    "            \n",
    "            # Save updated engine\n",
    "            save_path = save_updated_engine(updated_components)\n",
    "            \n",
    "            # Reload the engine with new documents\n",
    "            self.engine, self.components = load_search_engine(save_path)\n",
    "            \n",
    "            logger.info(f\"Successfully added {len(file_paths)} documents to the search engine\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error uploading documents: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def get_available_categories(self):\n",
    "        \"\"\"Get list of available document categories.\"\"\"\n",
    "        if self.engine is None or self.components is None:\n",
    "            return []\n",
    "        \n",
    "        chunks_df = self.components.get('chunks_df')\n",
    "        if chunks_df is None:\n",
    "            return []\n",
    "            \n",
    "        return sorted(chunks_df['category'].unique().tolist())\n",
    "    \n",
    "    def get_document_stats(self):\n",
    "        \"\"\"Get statistics about the documents in the search engine.\"\"\"\n",
    "        if self.engine is None or self.components is None:\n",
    "            return {}\n",
    "        \n",
    "        chunks_df = self.components.get('chunks_df')\n",
    "        if chunks_df is None:\n",
    "            return {}\n",
    "        \n",
    "        # Count documents per category\n",
    "        doc_counts = chunks_df.groupby('category')['doc_id'].nunique().to_dict()\n",
    "        \n",
    "        # Count chunks per category\n",
    "        chunk_counts = chunks_df['category'].value_counts().to_dict()\n",
    "        \n",
    "        # Total stats\n",
    "        stats = {\n",
    "            'total_documents': chunks_df['doc_id'].nunique(),\n",
    "            'total_chunks': len(chunks_df),\n",
    "            'categories': self.get_available_categories(),\n",
    "            'documents_per_category': doc_counts,\n",
    "            'chunks_per_category': chunk_counts,\n",
    "            'index_size': self.components.get('index').ntotal if self.components.get('index') else 0\n",
    "        }\n",
    "        \n",
    "        return stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5a7e55",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 2. Example Usage in a New Notebook\n",
    "\n",
    "Here's how would use this implementation in a new notebook:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b293132f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the upgraded search engine\n",
    "from document_upload_system import UpgradedSearchEngine\n",
    "\n",
    "# Initialize the search engine\n",
    "search_engine = UpgradedSearchEngine('../optimized_search_engine')\n",
    "\n",
    "# Display available categories\n",
    "print(\"Current document categories:\")\n",
    "print(search_engine.get_available_categories())\n",
    "\n",
    "# View statistics about the current documents\n",
    "stats = search_engine.get_document_stats()\n",
    "print(f\"\\nTotal documents: {stats['total_documents']}\")\n",
    "print(f\"Total chunks: {stats['total_chunks']}\")\n",
    "print(\"\\nDocuments per category:\")\n",
    "for category, count in stats['documents_per_category'].items():\n",
    "    print(f\"  {category}: {count}\")\n",
    "\n",
    "# Upload new documents\n",
    "new_files = [\n",
    "    'C:/path/to/New_Employment_Contract.docx',\n",
    "    'C:/path/to/Updated_Privacy_Policy.docx',\n",
    "    'C:/path/to/Vendor_Agreement_2025.docx'\n",
    "]\n",
    "\n",
    "new_categories = [\n",
    "    'Employment Contracts',\n",
    "    'Privacy Policies',\n",
    "    'Commercial Agreements'\n",
    "]\n",
    "\n",
    "success = search_engine.upload_new_documents(new_files, new_categories)\n",
    "if success:\n",
    "    print(\"\\nDocuments successfully added!\")\n",
    "    \n",
    "    # View updated statistics\n",
    "    updated_stats = search_engine.get_document_stats()\n",
    "    print(f\"\\nUpdated total documents: {updated_stats['total_documents']}\")\n",
    "    print(f\"Updated total chunks: {updated_stats['total_chunks']}\")\n",
    "    \n",
    "    # Search including the new documents\n",
    "    query = \"What are the new privacy requirements for customer data?\"\n",
    "    results = search_engine.search(query, auto_select=True)\n",
    "    \n",
    "    print(f\"\\nSearch results for '{query}':\")\n",
    "    display(results)\n",
    "else:\n",
    "    print(\"Failed to add documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d9126b2",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 3. Additional Features Can Implement\n",
    "\n",
    "1. **Document Versioning**: Track document versions when updated versions of the same document are uploaded.\n",
    "\n",
    "2. **Scheduled Indexing**: Set up a background process that checks a designated folder and automatically indexes new documents.\n",
    "\n",
    "3. **Document Validation**: Add validation checks to ensure documents meet specific criteria before being added to the index.\n",
    "\n",
    "4. **Incremental Updates**: Optimize the process to only update what's necessary when adding new documents.\n",
    "\n",
    "5. **Web Interface**: Create a simple web interface using Streamlit or Gradio for drag-and-drop document uploading.\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
